"""Module chatbot pour interroger les donnÃ©es."""
import pandas as pd
from litellm import completion
import os
import datetime
import streamlit as st

class DataChatbot:
    """Chatbot hybride (Cloud + Local) pour interroger les donnÃ©es."""
    
    def __init__(self, df: pd.DataFrame, 
                 primary_model: str = "gemini/gemini-2.5-flash-lite",
                 fallback_model: str = "ollama/mistral"):
        self.df = df
        self.primary_model = primary_model
        self.fallback_model = fallback_model # ModÃ¨le local (ex: mistral, llama3)
        self.context = self._build_context()
        self.history = []
    
    def _build_context(self) -> str:
        """Construit le contexte avec la date rÃ©elle du systÃ¨me."""
        
        # 1. PrÃ©paration des donnÃ©es brutes
        if len(self.df) > 1000:
            data_str = self.df.head(1000).to_string(index=False)
            warning = "(DonnÃ©es partielles : 1000 premiÃ¨res lignes)"
        else:
            data_str = self.df.to_string(index=False)
            warning = "(DonnÃ©es complÃ¨tes)"
            
        # 2. Date rÃ©elle du systÃ¨me (Dynamique)
        today_str = datetime.datetime.now().strftime("%d/%m/%Y")

        return f"""
        Tu es un expert mÃ©tÃ©orologue.
        
        CONTEXTE TEMPOREL :
        Nous sommes le {today_str} (Date du jour).
        
        DONNÃ‰ES DISPONIBLES {warning} :
        {data_str}
        
        TES MISSIONS :
        1. RÃ©ponds aux questions en utilisant UNIQUEMENT ce tableau.
        2. Si on te demande "demain", calcule la date par rapport Ã  aujourd'hui ({today_str}).
        3. IMPORTANT : Si les dates du tableau sont dans le futur par rapport Ã  aujourd'hui, considÃ¨re-les comme des prÃ©visions fiables Ã  long terme. Ne refuse pas de rÃ©pondre.
        4. Si les dates sont dans le passÃ©, considÃ¨re-les comme de l'historique.
        """
    
    def chat(self, user_message: str) -> str:
        messages = [{"role": "system", "content": self.context}]
        messages.extend(self.history)
        messages.append({"role": "user", "content": user_message})
        
        try:
            # 1. Tentative avec le modÃ¨le principal (Gemini)
            response = completion(
                model=self.primary_model,
                messages=messages,
                api_key=os.environ.get("GEMINI_API_KEY")
            )
            assistant_message = response.choices[0].message.content
            
        except Exception as e:
            # 2. FALLBACK sur Ollama en cas d'erreur (Quota, Connexion...)
            print(f"âš ï¸ Erreur Gemini ({str(e)}). Bascule sur Ollama ({self.fallback_model})...")
            
            # Notification discrÃ¨te dans l'interface
            st.toast(f"Mode hors-ligne activÃ© : Utilisation de {self.fallback_model}", icon="ğŸ›Ÿ")
            
            try:
                # Configuration pour Ollama local (port standard 11434)
                response = completion(
                    model=self.fallback_model,
                    messages=messages,
                    api_base="http://localhost:11434"
                )
                assistant_message = response.choices[0].message.content
            except Exception as e_local:
                return f"âŒ Erreur totale (Cloud & Local) : {str(e_local)}. VÃ©rifiez qu'Ollama est bien lancÃ©."

        # Mise Ã  jour de l'historique
        self.history.append({"role": "user", "content": user_message})
        self.history.append({"role": "assistant", "content": assistant_message})
        if len(self.history) > 20:
            self.history = self.history[-20:]
        
        return assistant_message
    
    def reset(self):
        self.history = []