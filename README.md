# ðŸŒ¤ï¸ TP3 â€” Application MÃ©tÃ©o Interactive & Chatbot IA

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://coursopendatatp3-53kkbcegsf5ay7i9dh9rvp.streamlit.app/)

**ðŸ”— DÃ©mo en ligne accessible ici : [Lancer l'application](https://coursopendatatp3-53kkbcegsf5ay7i9dh9rvp.streamlit.app/)**

Une application Data interactive (disponible en versions **Streamlit** et **Gradio**) permettant d'explorer des donnÃ©es mÃ©tÃ©orologiques enrichies et de dialoguer avec elles via un assistant IA hybride (RAG).

## âœ¨ FonctionnalitÃ©s

* **ðŸ“Š Dashboard Interactif :** Visualisation des indicateurs clÃ©s (KPIs) et explorateur de donnÃ©es.
* **ðŸ—ºï¸ Carte MÃ©tÃ©o :** Carte thermique interactive (Plotly) des tempÃ©ratures par ville.
* **ðŸ“ˆ PrÃ©visions DÃ©taillÃ©es :** Comparaison graphique des tempÃ©ratures entre plusieurs villes.
* **ðŸ¤– Assistant IA Hybride :** Chatbot intÃ©grÃ© pour interroger les donnÃ©es en langage naturel (*"Quelle ville sera la plus chaude ?"*).
    * **Mode Cloud :** Google Gemini 2.0 Flash (Performance).
    * **Mode Local (Fallback) :** Ollama / Mistral (RÃ©silience).
    * **Simulation Temporelle :** L'IA gÃ¨re les dates futures des prÃ©visions.

## ðŸ› ï¸ PrÃ©requis

* **Python 3.10+**
* **UV** (Gestionnaire de paquets moderne)
* **ClÃ© API Gemini** (Pour le mode Cloud)
* **Ollama** (Pour le mode Local - optionnel)

## ðŸ“¦ Installation (Local)

### 1. Cloner le projet

``` bash
git clone https://github.com/MP-Death30/Cours_Open_Data_tp3.git
cd tp3-app
```

### 2. Installer les dÃ©pendances

``` bash
uv sync

# Ou via pip classique :
# pip install -r requirements.txt
```

### 3. Configurer l'environnement
CrÃ©ez un fichier `.env` Ã  la racine et ajoutez votre clÃ© API :

``` env
GEMINI_API_KEY="votre_clÃ©_api_ici"
```

### 4. Import des DonnÃ©es
Copiez le fichier Parquet gÃ©nÃ©rÃ© par le pipeline TP2 dans le dossier `data/processed/`.

``` text
data/
â””â”€â”€ processed/
    â””â”€â”€ meteo_enriched_2025XXXX_XXXXXX.parquet
```

## ðŸš€ Utilisation (Local)

Vous avez le choix entre deux interfaces :

### Option A : Interface Streamlit (RecommandÃ© pour Dashboards)

``` bash
uv run streamlit run app_streamlit.py
```
*Accessible sur : `http://localhost:8501`*

### Option B : Interface Gradio (RecommandÃ© pour DÃ©mos ML)

``` bash
uv run python app_gradio.py
```
*Accessible sur : `http://127.0.0.1:7860`*

## ðŸŒ DÃ©ploiement (Streamlit Cloud)

L'application est dÃ©ployÃ©e et accessible publiquement.

Pour mettre Ã  jour ou dÃ©ployer votre propre version :

### 1. PrÃ©parer les dÃ©pendances
GÃ©nÃ©rez le fichier `requirements.txt` indispensable pour le cloud :

``` bash
uv export --format requirements-txt > requirements.txt
```

### 2. Mettre Ã  jour GitHub
Poussez votre code (y compris le fichier `requirements.txt`) :

``` bash
git add .
git commit -m "Prep: Ajout requirements.txt pour dÃ©ploiement"
git push origin main
```

### 3. DÃ©ployer sur Streamlit Cloud
1. Connectez-vous sur [share.streamlit.io](https://share.streamlit.io).
2. CrÃ©ez une **New app** liÃ©e Ã  votre dÃ©pÃ´t GitHub.
3. **IMPORTANT :** Avant de cliquer sur "Deploy", allez dans **Advanced settings** > **Secrets** et ajoutez votre clÃ© API :

``` toml
GEMINI_API_KEY = "votre_clÃ©_api_ici"
```

## ðŸ“‚ Architecture du Projet

``` text
tp3-app/
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml      # ThÃ¨me personnalisÃ© Streamlit
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/       # DonnÃ©es mÃ©tÃ©orologiques (Parquet)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ charts.py        # Visualisations Plotly partagÃ©es
â”‚   â”œâ”€â”€ chatbot.py       # Moteur IA (Gemini + Ollama + Context)
â”‚   â””â”€â”€ data.py          # Chargement DuckDB & Filtrage
â”œâ”€â”€ app_streamlit.py     # Application Principale (Streamlit)
â”œâ”€â”€ app_gradio.py        # Application Alternative (Gradio)
â”œâ”€â”€ requirements.txt     # DÃ©pendances pour le Cloud
â”œâ”€â”€ pyproject.toml       # DÃ©pendances (UV)
â”œâ”€â”€ README.md            # Documentation
â””â”€â”€ .env                 # Secrets (non versionnÃ©)
```

## ðŸ¤– Configuration du Chatbot

Le chatbot utilise une stratÃ©gie de **redondance** :

1.  **Tentative Principale :** Connexion Ã  l'API **Gemini** (rapide, nÃ©cessite clÃ© API).
2.  **Fallback Automatique :** En cas d'erreur ou de coupure internet, bascule sur **Ollama** en local (`http://localhost:11434`).

Pour activer le mode local, assurez-vous qu'Ollama est lancÃ© :

``` bash
ollama pull mistral
```

## ðŸ‘¤ Auteur

Projet rÃ©alisÃ© dans le cadre du module **Open Data & IA**.