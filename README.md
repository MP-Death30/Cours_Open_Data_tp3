# ğŸŒ¤ï¸ TP3 â€” Application MÃ©tÃ©o Interactive & Chatbot IA

Une application Data interactive (disponible en versions **Streamlit** et **Gradio**) permettant d'explorer des donnÃ©es mÃ©tÃ©orologiques enrichies et de dialoguer avec elles via un assistant IA hybride (RAG).

## âœ¨ FonctionnalitÃ©s

* **ğŸ“Š Dashboard Interactif :** Visualisation des indicateurs clÃ©s (KPIs) et explorateur de donnÃ©es.
* **ğŸ—ºï¸ Carte MÃ©tÃ©o :** Carte thermique interactive (Plotly) des tempÃ©ratures par ville.
* **ğŸ“ˆ PrÃ©visions DÃ©taillÃ©es :** Comparaison graphique des tempÃ©ratures entre plusieurs villes.
* **ğŸ¤– Assistant IA Hybride :** Chatbot intÃ©grÃ© pour interroger les donnÃ©es en langage naturel (*"Quelle ville sera la plus chaude ?"*).
    * **Mode Cloud :** Google Gemini 2.0 Flash (Performance).
    * **Mode Local (Fallback) :** Ollama / Mistral (RÃ©silience).
    * **Simulation Temporelle :** L'IA gÃ¨re les dates futures des prÃ©visions.

## ğŸ› ï¸ PrÃ©requis

* **Python 3.10+**
* **UV** (Gestionnaire de paquets moderne)
* **ClÃ© API Gemini** (Pour le mode Cloud)
* **Ollama** (Pour le mode Local - optionnel)

## ğŸ“¦ Installation

### 1. Cloner le projet

``` bash
git clone https://github.com/MP-Death30/Cours_Open_Data_tp3.git
cd tp3-app
```

### 2. Installer les dÃ©pendances

``` bash
uv sync

# Ou via pip classique :
# pip install -r requirements.txt
```

### 3. Configurer l'environnement
CrÃ©ez un fichier `.env` Ã  la racine et ajoutez votre clÃ© API :

``` env
GEMINI_API_KEY="votre_clÃ©_api_ici"
```

### 4. Import des DonnÃ©es
Copiez le fichier Parquet gÃ©nÃ©rÃ© par le pipeline TP2 dans le dossier `data/processed/`.

``` text
data/
â””â”€â”€ processed/
    â””â”€â”€ meteo_enriched_2025XXXX_XXXXXX.parquet
```

## ğŸš€ Utilisation

Vous avez le choix entre deux interfaces :

### Option A : Interface Streamlit (RecommandÃ© pour Dashboards)

``` bash
uv run streamlit run app_streamlit.py
```
*Accessible sur : `http://localhost:8501`*

### Option B : Interface Gradio (RecommandÃ© pour DÃ©mos ML)

``` bash
uv run python app_gradio.py
```
*Accessible sur : `http://127.0.0.1:7860`*

## ğŸ“‚ Architecture du Projet

``` text
tp3-app/
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml      # ThÃ¨me personnalisÃ© Streamlit
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/       # DonnÃ©es mÃ©tÃ©orologiques (Parquet)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ charts.py        # Visualisations Plotly partagÃ©es
â”‚   â”œâ”€â”€ chatbot.py       # Moteur IA (Gemini + Ollama + Context)
â”‚   â””â”€â”€ data.py          # Chargement DuckDB & Filtrage
â”œâ”€â”€ app_streamlit.py     # Application Principale (Streamlit)
â”œâ”€â”€ app_gradio.py        # Application Alternative (Gradio)
â”œâ”€â”€ pyproject.toml       # DÃ©pendances (UV)
â”œâ”€â”€ README.md            # Documentation
â””â”€â”€ .env                 # Secrets (non versionnÃ©)
```

## ğŸ¤– Configuration du Chatbot

Le chatbot utilise une stratÃ©gie de **redondance** :

1.  **Tentative Principale :** Connexion Ã  l'API **Gemini** (rapide, nÃ©cessite clÃ© API).
2.  **Fallback Automatique :** En cas d'erreur ou de coupure internet, bascule sur **Ollama** en local (`http://localhost:11434`).

Pour activer le mode local, assurez-vous qu'Ollama est lancÃ© :

``` bash
ollama pull mistral
```

## ğŸ‘¤ Auteur

Projet rÃ©alisÃ© dans le cadre du module **Open Data & IA**.